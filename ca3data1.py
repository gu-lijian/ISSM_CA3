# -*- coding: utf-8 -*-
"""CA3data1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yiYoWA1sOAuClB_WKXB4QwbQwWdww8oT
"""

# Commented out IPython magic to ensure Python compatibility.
# Load necessary packages
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import pywt
import pandas as pd
import time
from sklearn.neural_network import MLPClassifier 
from sklearn.metrics import confusion_matrix
from warnings import filterwarnings # Avoid training warning message in MLPClassifier
filterwarnings('ignore')
import time
import csv
from enum import Enum
from scipy.signal import butter, filtfilt


WINDOW_SIZE = 10 * 30 - 15
DURATION = 30
VERBOSE = False
SECONDS_PER_HOUR = 3600
SECONDS_PER_DAY = 3600 * 24

Path_to_file = 'D:\GoogleDrive\MTech\Courses\ISY5002 Pattern Recognition Systems\Intelligent Sensing & Sense Making\CA3\sleep_classifiers-master\data\\'
Path_to_clearned_file = 'D:\GoogleDrive\MTech\Courses\ISY5002 Pattern Recognition Systems\Intelligent Sensing & Sense Making\CA3\sleep_classifiers-master\cleaneddata\\'

def get_all_subject_ids():
        subjects_as_ints = [1360686]

        '''subjects_as_ints = [3509524, 5132496, 1066528, 5498603, 2638030, 2598705, 5383425, 1455390, 4018081, 9961348,
                            1449548, 8258170, 781756, 9106476, 8686948, 8530312, 3997827, 4314139, 1818471, 4426783,
                            8173033, 7749105, 5797046, 759667, 8000685, 6220552, 844359, 9618981, 1360686, 46343,
                            8692923] '''

        subjects_as_strings = []

        for subject in subjects_as_ints:
            subjects_as_strings.append(str(subject))
        return subjects_as_strings

subject_ids = get_all_subject_ids()

class Epoch(object):
    DURATION = 30  # seconds

    def __init__(self, timestamp, index):
        self.timestamp = timestamp
        self.index = index

class SleepStage(Enum):
    wake = 0
    n1 = 1
    n2 = 2
    n3 = 3
    n4 = 4
    rem = 5
    unscored = -1

class StageItem(object):
    def __init__(self, epoch, stage: SleepStage):
        self.epoch = epoch
        self.stage = stage

class PSGConverter(object):
    strings_to_labels = {
        "?": SleepStage.unscored,
        "W": SleepStage.wake,
        "1": SleepStage.n1,
        "N1": SleepStage.n1,
        "2": SleepStage.n2,
        "N2": SleepStage.n2,
        "3": SleepStage.n3,
        "N3": SleepStage.n3,
        "4": SleepStage.n4,
        "N4": SleepStage.n4,
        "R": SleepStage.rem,
        "M": SleepStage.wake}

    ints_to_labels = {
        -1: SleepStage.unscored,
        0: SleepStage.wake,
        1: SleepStage.n1,
        2: SleepStage.n2,
        3: SleepStage.n3,
        4: SleepStage.n4,
        5: SleepStage.rem,
        6: SleepStage.unscored}

    @staticmethod
    def get_label_from_string(stage_string):
        if stage_string in PSGConverter.strings_to_labels:
            return PSGConverter.strings_to_labels[stage_string]

    @staticmethod
    def get_label_from_int(stage_int):
        if stage_int in PSGConverter.ints_to_labels:
            return PSGConverter.ints_to_labels[stage_int]

class Interval(object):
    def __init__(self, start_time, end_time):
        self.start_time = start_time
        self.end_time = end_time

class PSGRawDataCollection(object):
    def __init__(self, subject_id, data: [SleepStage]):
        self.subject_id = subject_id
        self.data = data

    def get_np_array(self):
        number_of_epochs = len(self.data)
        array = np.zeros((number_of_epochs, 2))

        for index in range(number_of_epochs):
            stage_item = self.data[index]
            array[index, 0] = stage_item.epoch.timestamp
            array[index, 1] = stage_item.stage.value

        return array

    def get_interval(self):
        number_of_epochs = len(self.data)
        min_timestamp = 1e15
        max_timestamp = -1

        for index in range(number_of_epochs):
            stage_item = self.data[index]
            if stage_item.epoch.timestamp < min_timestamp:
                min_timestamp = stage_item.epoch.timestamp
            if stage_item.epoch.timestamp > max_timestamp:
                max_timestamp = stage_item.epoch.timestamp

        return Interval(start_time=min_timestamp, end_time=max_timestamp)

def read_precleaned(subject_id):
        # psg_path = 'data/labels/' + str(subject_id) + '_labeled_sleep.txt'
        psg_path = Path_to_file + subject_id + '_labeled_sleep.txt'
        data = []

        with open(psg_path, 'rt') as csv_file:
            file_reader = csv.reader(csv_file, delimiter=' ', quotechar='|')
            count = 0
            rows_per_epoch = 1
            for row in file_reader:
                if count == 0:
                    start_time = float(row[0])
                    start_score = int(row[1])
                    epoch = Epoch(timestamp=start_time, index=1)
                    data.append(StageItem(epoch=epoch, stage=PSGConverter.get_label_from_int(start_score)))
                else:
                    timestamp = start_time + count * 30
                    score = int(row[1])
                    epoch = Epoch(timestamp=timestamp,
                                  index=(1 + int(np.floor(count / rows_per_epoch))))

                    data.append(StageItem(epoch=epoch, stage=PSGConverter.get_label_from_int(score)))
                count = count + 1
        return PSGRawDataCollection(subject_id=subject_id, data=data)

class MotionCollection(object):
    def __init__(self, subject_id, data):
        self.subject_id = subject_id
        self.data = data
        self.timestamps = data[:, 0]
        self.values = data[:, 1:]

    def get_interval(self):
        return Interval(start_time=np.amin(self.data[:, 0]),
                        end_time=np.amax(self.data[:, 0]))

class HeartRateCollection(object):
    def __init__(self, subject_id, data):
        self.subject_id = subject_id
        self.data = data
        self.timestamps = data[:, 0]
        self.values = data[:, 1:]

    def get_interval(self):
        return Interval(start_time=np.amin(self.data[:, 0]),
                        end_time=np.amax(self.data[:, 0]))

def load(file, delimiter=' '):
        motion_array = pd.read_csv(str(file), delimiter=delimiter).values
        return motion_array

def remove_repeats(array):
    array_no_repeats = np.unique(array, axis=0)
    array_no_repeats = array_no_repeats[np.argsort(array_no_repeats[:, 0])]
    return array_no_repeats

def motion_load_raw(subject_id):
  raw_motion_path = Path_to_file + subject_id + '_acceleration.txt'
  motion_array = load(raw_motion_path)
  motion_array = remove_repeats(motion_array)
  return MotionCollection(subject_id=subject_id, data=motion_array)

def heartrate_load_raw(subject_id):
  raw_hr_path = Path_to_file + subject_id + '_heartrate.txt'
  heart_rate_array = load(raw_hr_path, ",")
  heart_rate_array = remove_repeats(heart_rate_array)
  return HeartRateCollection(subject_id=subject_id, data=heart_rate_array)

def get_intersecting_interval(collection_list):
        start_times = []
        end_times = []
        for collection in collection_list:
            interval = collection.get_interval()
            start_times.append(interval.start_time)
            end_times.append(interval.end_time)

        return Interval(start_time=max(start_times), end_time=min(end_times))

def psg_crop(psg_raw_collection, interval):
        subject_id = psg_raw_collection.subject_id

        stage_items = []
        for stage_item in psg_raw_collection.data:
            timestamp = stage_item.epoch.timestamp
            if interval.start_time <= timestamp < interval.end_time:
                stage_items.append(stage_item)
        return PSGRawDataCollection(subject_id=subject_id, data=stage_items)

def motion_crop(motion_collection, interval):
        subject_id = motion_collection.subject_id
        timestamps = motion_collection.timestamps
        valid_indices = ((timestamps >= interval.start_time)
                         & (timestamps < interval.end_time)).nonzero()[0]

        cropped_data = motion_collection.data[valid_indices, :]
        return MotionCollection(subject_id=subject_id, data=cropped_data)

def heartrate_crop(heart_rate_collection, interval):
        subject_id = heart_rate_collection.subject_id
        timestamps = heart_rate_collection.timestamps
        valid_indices = ((timestamps >= interval.start_time)
                         & (timestamps < interval.end_time)).nonzero()[0]

        cropped_data = heart_rate_collection.data[valid_indices, :]
        return HeartRateCollection(subject_id=subject_id, data=cropped_data)

def psg_write(psg_raw_data_collection, subject_id):
        data_array = []
        s = ''
        f2 = open(Path_to_clearned_file + subject_id + '_cleaned_psg.csv', 'a+')

        for index in range(len(psg_raw_data_collection.data)):
            stage_item = psg_raw_data_collection.data[index]
            data_array.append([stage_item.epoch.timestamp, stage_item.stage.value])
            s = str(stage_item.epoch.timestamp) + ',' + str(stage_item.stage.value)
            f2.write(s+'\n')

        np_psg_array = np.array(data_array)
        psg_output_path = Path_to_clearned_file + subject_id + '_cleaned_psg.txt'

        np.savetxt(psg_output_path, np_psg_array, fmt='%f')

        
        f2.close()

def psg_write_label(subject_id, labels):
        psg_labels_path = Path_to_clearned_file + subject_id + '_psg_labels.txt'
        np.savetxt(psg_labels_path, labels, fmt='%f')

def motion_write(motion_collection, subject_id):
        motion_output_path = Path_to_clearned_file + subject_id + '_cleaned_motion.txt'
        np.savetxt(motion_output_path, motion_collection.data, fmt='%f')

def heartrate_write(heart_rate_collection, subject_id):
        hr_output_path = Path_to_clearned_file + subject_id + '_cleaned_heartrate.txt'
        np.savetxt(hr_output_path, heart_rate_collection.data, fmt='%f')

def max2epochs(data, fs, epoch):
        data = data.flatten()

        seconds = int(np.floor(np.shape(data)[0] / fs))
        data = np.abs(data)
        data = data[0:int(seconds * fs)]

        data = data.reshape(fs, seconds, order='F').copy()

        data = data.max(0)
        data = data.flatten()
        N = np.shape(data)[0]
        num_epochs = int(np.floor(N / epoch))
        data = data[0:(num_epochs * epoch)]

        data = data.reshape(epoch, num_epochs, order='F').copy()
        epoch_data = np.sum(data, axis=0)
        epoch_data = epoch_data.flatten()

        return epoch_data

def build_activity_counts_without_matlab(subject_id, data):

        fs = 50
        time = np.arange(np.amin(data[:, 0]), np.amax(data[:, 0]), 1.0 / fs)
        z_data = np.interp(time, data[:, 0], data[:, 3])

        cf_low = 3
        cf_hi = 11
        order = 5
        w1 = cf_low / (fs / 2)
        w2 = cf_hi / (fs / 2)
        pass_band = [w1, w2]
        b, a = butter(order, pass_band, 'bandpass')

        z_filt = filtfilt(b, a, z_data)
        z_filt = np.abs(z_filt)
        top_edge = 5
        bottom_edge = 0
        number_of_bins = 128

        bin_edges = np.linspace(bottom_edge, top_edge, number_of_bins + 1)
        binned = np.digitize(z_filt, bin_edges)
        epoch = 15
        counts = max2epochs(binned, fs, epoch)
        counts = (counts - 18) * 3.07
        counts[counts < 0] = 0

        time_counts = np.linspace(np.min(data[:, 0]), max(data[:, 0]), np.shape(counts)[0])
        time_counts = np.expand_dims(time_counts, axis=1)
        counts = np.expand_dims(counts, axis=1)
        output = np.hstack((time_counts, counts))

        activity_count_output_path = Path_to_clearned_file + subject_id + '_cleaned_counts.txt'
        np.savetxt(activity_count_output_path, output, fmt='%f', delimiter=',')

def txt_csv_motion(subject_id):
  f2 = open(Path_to_clearned_file + subject_id + '_cleaned_motion.csv','a+')
  with open(Path_to_clearned_file + subject_id + '_cleaned_motion.txt') as f:
    for line in f.readlines():
      f2.write(line.replace(' ',','))
  f2.close()

def txt_csv_heartrate(subject_id):
  f2 = open(Path_to_clearned_file + subject_id + '_cleaned_heartrate.csv','a+')
  with open(Path_to_clearned_file + subject_id + '_cleaned_heartrate.txt') as f:
    for line in f.readlines():
      f2.write(line.replace(' ',','))
  f2.close()

def crop_all(subject_id):
        # psg_raw_collection = PSGService.read_raw(subject_id)  # Used to extract PSG details from the reports
        psg_raw_collection = read_precleaned(subject_id)  # Loads already extracted PSG data
        motion_collection = motion_load_raw(subject_id)
        heart_rate_collection = heartrate_load_raw(subject_id)
        valid_interval = get_intersecting_interval([psg_raw_collection,
                                                                     motion_collection,
                                                                     heart_rate_collection])
        
        psg_raw_collection = psg_crop(psg_raw_collection, valid_interval)
        
        
        motion_collection = motion_crop(motion_collection, valid_interval)
        heart_rate_collection = heartrate_crop(heart_rate_collection, valid_interval)


        psg_write(psg_raw_collection, subject_id)
        motion_write(motion_collection, subject_id)
        heartrate_write(heart_rate_collection, subject_id)
       
        build_activity_counts_without_matlab(subject_id, motion_collection.data)  # Builds activity counts with python, not MATLAB
        
        txt_csv_heartrate(subject_id)
        txt_csv_motion(subject_id)

def psg_load_cropped_array(subject_id):
  cropped_psg_path = Path_to_clearned_file +subject_id + '_cleaned_psg.txt'
  return pd.read_csv(str(cropped_psg_path), delimiter=' ').values

def psg_load_cropped(subject_id):
        cropped_array = psg_load_cropped_array(subject_id)
        stage_items = []

        for row in range(np.shape(cropped_array)[0]):
            value = cropped_array[row, 1]
            stage_items.append(StageItem(epoch=Epoch(timestamp=cropped_array[row, 0], index=row),
                                         stage=PSGConverter.get_label_from_int(value)))

        return PSGRawDataCollection(subject_id=subject_id, data=stage_items)

def heartrate_load_cropped(subject_id):
  cropped_hr_path = Path_to_clearned_file +subject_id + '_cleaned_heartrate.txt'
  heart_rate_array = load(cropped_hr_path)
  return HeartRateCollection(subject_id=subject_id, data=heart_rate_array)

def motion_load_cropped(subject_id):
  cropped_motion_path = Path_to_clearned_file + subject_id + '_cleaned_motion.txt'
  motion_array = load(cropped_motion_path)
  return MotionCollection(subject_id=subject_id, data=motion_array)

def psg_build(subject_id, valid_epochs):
        psg_array = psg_load_cropped_array(subject_id)
        labels = []
        for epoch in valid_epochs:
            value = np.interp(epoch.timestamp, psg_array[:, 0], psg_array[:, 1])
            labels.append(value)
        return np.array(labels)

def build_labels(subject_id, valid_epochs):
  psg_labels = psg_build(subject_id, valid_epochs)
  psg_write_label(subject_id, psg_labels)

def convolve_with_dog(y, box_pts):
    y = y - np.mean(y)
    box = np.ones(box_pts) / box_pts

    mu1 = int(box_pts / 2.0)
    sigma1 = 120

    mu2 = int(box_pts / 2.0)
    sigma2 = 600

    scalar = 0.75

    for ind in range(0, box_pts):
        box[ind] = np.exp(-1 / 2 * (((ind - mu1) / sigma1) ** 2)) - scalar * np.exp(
            -1 / 2 * (((ind - mu2) / sigma2) ** 2))

    y = np.insert(y, 0, np.flip(y[0:int(box_pts / 2)]))  # Pad by repeating boundary conditions
    y = np.insert(y, len(y) - 1, np.flip(y[int(-box_pts / 2):]))
    y_smooth = np.convolve(y, box, mode='valid')

    return y_smooth

def interpolate_and_normalize(heart_rate_collection):
  timestamps = heart_rate_collection.timestamps.flatten()
  heart_rate_values = heart_rate_collection.values.flatten()
  interpolated_timestamps = np.arange(np.amin(timestamps),
                                            np.amax(timestamps), 1)
  interpolated_hr = np.interp(interpolated_timestamps, timestamps, heart_rate_values)
  interpolated_hr = convolve_with_dog(interpolated_hr, WINDOW_SIZE)
  scalar = np.percentile(np.abs(interpolated_hr), 90)
  interpolated_hr = interpolated_hr / scalar

  return interpolated_timestamps, interpolated_hr

def get_window(timestamps, epoch):
        start_time = epoch.timestamp - WINDOW_SIZE
        end_time = epoch.timestamp + DURATION + WINDOW_SIZE
        timestamps_ravel = timestamps.ravel()
        indices_in_range = np.unravel_index(np.where((timestamps_ravel > start_time) & (timestamps_ravel < end_time)),
                                            timestamps.shape)
        return indices_in_range[0][0]

def get_feature(heart_rate_values):
  return [np.std(heart_rate_values)]

def heartrate_build_from_collection(heart_rate_collection, valid_epochs):
        heart_rate_features = []

        interpolated_timestamps, interpolated_hr = interpolate_and_normalize(
            heart_rate_collection)

        for epoch in valid_epochs:
            indices_in_range = get_window(interpolated_timestamps, epoch)
            heart_rate_values_in_range = interpolated_hr[indices_in_range]

            feature = get_feature(heart_rate_values_in_range)

            heart_rate_features.append(feature)

        return np.array(heart_rate_features)

def heartrate_build(subject_id, valid_epochs):
        heart_rate_collection = heartrate_load_cropped(subject_id)
        return heartrate_build_from_collection(heart_rate_collection, valid_epochs)

def count_feature_write(subject_id, feature):
        activity_counts_feature_path = Path_to_clearned_file + subject_id + '_count_feature.txt'
        np.savetxt(activity_counts_feature_path, feature, fmt='%f')

def heartrate_feature_write(subject_id, feature):
        heart_rate_feature_path = Path_to_clearned_file + subject_id + '_heartrate_feature.txt'
        np.savetxt(heart_rate_feature_path, feature, fmt='%f')

def count_load(counts_file):
        counts_array = pd.read_csv(str(counts_file)).values
        return counts_array

class ActivityCountCollection(object):
    def __init__(self, subject_id, data):
        self.subject_id = subject_id
        self.data = data
        self.timestamps = data[:, 0]
        self.values = data[:, 1:]

    def get_interval(self):
        return Interval(start_time=np.amin(self.data[:, 0]),
                        end_time=np.amax(self.data[:, 0]))

def count_load_cropped(subject_id):
        activity_counts_path = Path_to_clearned_file + subject_id + '_cleaned_counts.txt'
        counts_array = count_load(activity_counts_path)
        return ActivityCountCollection(subject_id=subject_id, data=counts_array)

def heartrate_load_cropped(subject_id):
  cropped_hr_path = Path_to_clearned_file + subject_id + '_cleaned_heartrate.txt'
  heart_rate_array = load(cropped_hr_path)
  return HeartRateCollection(subject_id=subject_id, data=heart_rate_array)

def count_interpolate(activity_count_collection):
        timestamps = activity_count_collection.timestamps.flatten()
        activity_count_values = activity_count_collection.values.flatten()
        interpolated_timestamps = np.arange(np.amin(timestamps),
                                            np.amax(timestamps), 1)
        interpolated_counts = np.interp(interpolated_timestamps, timestamps, activity_count_values)
        return interpolated_timestamps, interpolated_counts

def smooth_gauss(y, box_pts):
    box = np.ones(box_pts) / box_pts
    mu = int(box_pts / 2.0)
    sigma = 50  # seconds

    for ind in range(0, box_pts):
        box[ind] = np.exp(-1 / 2 * (((ind - mu) / sigma) ** 2))

    box = box / np.sum(box)
    sum_value = 0
    for ind in range(0, box_pts):
        sum_value += box[ind] * y[ind]

    return sum_value

def count_get_feature(count_values):
        convolution = smooth_gauss(count_values.flatten(), np.shape(count_values.flatten())[0])
        return np.array([convolution])

def count_build_from_collection(activity_count_collection, valid_epochs):
        count_features = []

        interpolated_timestamps, interpolated_counts = count_interpolate(
            activity_count_collection)

        for epoch in valid_epochs:
            indices_in_range = get_window(interpolated_timestamps, epoch)
            activity_counts_in_range = interpolated_counts[indices_in_range]

            feature = count_get_feature(activity_counts_in_range)
            count_features.append(feature)

        return np.array(count_features)

def count_build(subject_id, valid_epochs):
  activity_count_collection = count_load_cropped(subject_id)
  return count_build_from_collection(activity_count_collection, valid_epochs)

def build_from_wearables(subject_id, valid_epochs):

        count_feature = count_build(subject_id, valid_epochs)
        heart_rate_feature = heartrate_build(subject_id, valid_epochs)
        count_feature_write(subject_id, count_feature)
        heartrate_feature_write(subject_id, heart_rate_feature)

def get_valid_epoch_dictionary(timestamps, start_time):
        epoch_dictionary = {}

        for ind in range(np.shape(timestamps)[0]):
            time = timestamps[ind]
            floored_timestamp = time - np.mod(time - start_time, DURATION)

            epoch_dictionary[floored_timestamp] = True

        return epoch_dictionary

def get_valid_epochs(subject_id):

        psg_collection = psg_load_cropped(subject_id)
        motion_collection = motion_load_cropped(subject_id)
        heart_rate_collection = heartrate_load_cropped(subject_id)

        start_time = psg_collection.data[0].epoch.timestamp
        motion_epoch_dictionary = get_valid_epoch_dictionary(motion_collection.timestamps,
                                                                              start_time)
        hr_epoch_dictionary = get_valid_epoch_dictionary(heart_rate_collection.timestamps,
                                                                          start_time)

        valid_epochs = []
        for stage_item in psg_collection.data:
            epoch = stage_item.epoch

            if epoch.timestamp in motion_epoch_dictionary and epoch.timestamp in hr_epoch_dictionary \
                    and stage_item.stage != SleepStage.unscored:
                valid_epochs.append(epoch)

        return valid_epochs

def cosine_proxy(time):
        sleep_drive_cosine_shift = 5
        return -1 * np.math.cos((time - sleep_drive_cosine_shift * SECONDS_PER_HOUR) *
                                2 * np.math.pi / SECONDS_PER_DAY)

def build_cosine(valid_epochs):
        features = []
        first_value = cosine_proxy(0)
        first_timestamp = valid_epochs[0].timestamp

        for epoch in valid_epochs:
            value = cosine_proxy(epoch.timestamp - first_timestamp)
            normalized_value = value
            features.append(normalized_value)

        return np.array(features)

def write_cosine(subject_id, feature):
        feature_path = Path_to_clearned_file + subject_id + '_cosine_feature.txt'
        np.savetxt(feature_path, feature, fmt='%f')

def build_time(valid_epochs):
        features = []
        first_timestamp = valid_epochs[0].timestamp
        for epoch in valid_epochs:
            value = epoch.timestamp - first_timestamp

            value = value / 3600.0  # Changing units to hours improves performance

            features.append(value)
        return np.array(features)

def build_from_time(subject_id, valid_epochs):
  cosine_feature = build_cosine(valid_epochs)
  time_feature = build_time(valid_epochs)

  write_cosine(subject_id, cosine_feature)
  write_time(subject_id, time_feature)

def write_time(subject_id, feature):
        feature_path = Path_to_clearned_file + subject_id + '_time_feature.txt'
        np.savetxt(feature_path, feature, fmt='%f')

def feature_build(subject_id):
        if VERBOSE:
            print("Getting valid epochs...")
        valid_epochs = get_valid_epochs(subject_id)

        if VERBOSE:
            print("Building features...")
        build_labels(subject_id, valid_epochs)
        build_from_wearables(subject_id, valid_epochs)
        build_from_time(subject_id, valid_epochs)

def run_preprocessing(subject_set):
    start_time = time.time()

    for subject in subject_set:
        print("Cropping data from subject " + str(subject) + "...")
        crop_all(str(subject))

    # ActivityCountService.build_activity_counts()  # This uses MATLAB, but has been replaced with a python implementation
    #CircadianService.build_circadian_model()
    #CircadianService.build_circadian_mesa()

    for subject in subject_set:
        feature_build(str(subject))

    end_time = time.time()
    print("Execution took " + str((end_time - start_time) / 60) + " minutes")

run_preprocessing(subject_ids)